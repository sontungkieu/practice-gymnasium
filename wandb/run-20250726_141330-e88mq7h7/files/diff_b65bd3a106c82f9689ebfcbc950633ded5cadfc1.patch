diff --git a/TRPO.py b/TRPO.py
index 525fdf8..7e71097 100644
--- a/TRPO.py
+++ b/TRPO.py
@@ -78,37 +78,6 @@ def parse_args():
     return args
 
 
-# def make_env(env_id, idx, capture_video, run_name, gamma):
-#     def thunk():
-#         if capture_video:
-#             env = gym.make(env_id, render_mode="rgb_array")
-#         else:
-#             env = gym.make(env_id)
-#         env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
-#         env = gym.wrappers.RecordEpisodeStatistics(env)
-#         if capture_video:
-#             if idx == 0:
-#                 env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
-#         env = gym.wrappers.ClipAction(env)
-#         env = gym.wrappers.NormalizeObservation(env)
-#         # Thêm clipping và cung cấp observation_space mới:
-#         orig_space = env.observation_space
-#         clipped_space = spaces.Box(
-#             low=-10.0,
-#             high=10.0,
-#             shape=orig_space.shape,
-#             dtype=orig_space.dtype
-#         )
-#         env = gym.wrappers.TransformObservation(
-#             env,
-#             func=lambda obs: np.clip(obs, -10, 10),
-#             observation_space=clipped_space
-#         )
-#         env = gym.wrappers.NormalizeReward(env, gamma=gamma)
-#         env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
-#         return env
-
-#     return thunk
 def make_env(env_id, idx, capture_video, run_name, gamma):
     def thunk():
         if capture_video:
@@ -122,12 +91,43 @@ def make_env(env_id, idx, capture_video, run_name, gamma):
                 env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
         env = gym.wrappers.ClipAction(env)
         env = gym.wrappers.NormalizeObservation(env)
-        env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
+        # Thêm clipping và cung cấp observation_space mới:
+        orig_space = env.observation_space
+        clipped_space = spaces.Box(
+            low=-10.0,
+            high=10.0,
+            shape=orig_space.shape,
+            dtype=orig_space.dtype
+        )
+        env = gym.wrappers.TransformObservation(
+            env,
+            func=lambda obs: np.clip(obs, -10, 10),
+            observation_space=clipped_space
+        )
         env = gym.wrappers.NormalizeReward(env, gamma=gamma)
         env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
         return env
 
     return thunk
+# def make_env(env_id, idx, capture_video, run_name, gamma):
+#     def thunk():
+#         if capture_video:
+#             env = gym.make(env_id, render_mode="rgb_array")
+#         else:
+#             env = gym.make(env_id)
+#         env = gym.wrappers.FlattenObservation(env)  # deal with dm_control's Dict observation space
+#         env = gym.wrappers.RecordEpisodeStatistics(env)
+#         if capture_video:
+#             if idx == 0:
+#                 env = gym.wrappers.RecordVideo(env, f"videos/{run_name}")
+#         env = gym.wrappers.ClipAction(env)
+#         env = gym.wrappers.NormalizeObservation(env)
+#         env = gym.wrappers.TransformObservation(env, lambda obs: np.clip(obs, -10, 10))
+#         env = gym.wrappers.NormalizeReward(env, gamma=gamma)
+#         env = gym.wrappers.TransformReward(env, lambda reward: np.clip(reward, -10, 10))
+#         return env
+
+#     return thunk
 
 
 def layer_init(layer, std=np.sqrt(2), bias_const=0.0):
diff --git a/wandb/latest-run b/wandb/latest-run
index c3b887f..1bb6508 120000
--- a/wandb/latest-run
+++ b/wandb/latest-run
@@ -1 +1 @@
-run-20250726_004947-lonc42jh
\ No newline at end of file
+run-20250726_141330-e88mq7h7
\ No newline at end of file
